
We chose to specifically examine the formation period of a Chord Hash. The cited papers discuss how a single node can join a Chord Hash but gloss over the initial state for building a Chord Hash from scratch in a distributed setting. This made general simulation of Chord difficult as at least an initial chord ring had to be "boot-strapped" as a pre-connected initial ring so that other nodes can join it. Because this was an architecture level design problem, we were not interested in parts of the Chord Hash technique like the behavior of the actual network topology, latency, or server capacity. Thus we make a series of simplifying assumptions. 
For this simulation we only consider the overlay topology, not the underlying topology of the network, thus we create links with the assumption that nodes have the potential to link into a clique (and in many small network cases the system does actually form a clique). We also ignore file lookup, as we are not concerned with this network’s ability to store files beyond being prepared to do so. Because hash values won’t be perfectly distributed, it is perfectly acceptable and expected to have duplicate entries in the finger table. We focus on what servers are connected to each other and where they think the rest of the chord ring is located. We allow ourselves to change the hash degree of the simulation simply to check that changing it does not change behavior, and in practice a value of 64bit, 128bit or higher should be used, which our simulation platform would not support in both integer handling and scale. We chose a maximum size of 20bit for the dual purpose of limiting the simulation scale and to avoid overflow errors in operations on hashes. If we were careful, it is theoretically possible for us to use as high as 31bit in NetLogo (as it does not support unsigned integer type) but we decided this gave us no benfit and would require a serious amount of effort. We model messages in this network as an agent of it's own. This is because a traveling agent "packet" gives us a model of latency and race conditions, two features that do not necessarily need to relate exactly to how they occur in reality but our technique must defend against in the general sense.

------------------------------------

Our experiment consisted of 200 simulation runs, 100 with our modified technique and 100 with the original algorithm. The number of links at every point in time for all 200 simulations was recorded, and above we graph the mean and variance of each algorithm over time.

Using metrics to discuss our success is difficult as our desired change is difficult. An obvious metric like the number of distinct sub-rings proves our technique works, but does not give us any information beyond that. In the unaltered simulation the number of rings stays constant and in the modified behavior it linearly reduces to one. The major metrics of number of links and average travel time both fall into normal ranges when the undesired behavior occurs. Each node has a constant number of links and the expected number of links is nodes * number of links * density. This number of links is unaffected by our undesired behavior as the total density of the ideal ring is equal to the sum of the densities of the sub-rings. Travel time for messages is unaffected as each sub-ring has sections that maintain the entire hash space, thus a message will be replied to, just by the wrong node.

The metrics we found to be of interest is the rate of link creation and variance in the number of links over time. We found after running 100 simulations for each algorithm the the "end behavior" of the number of links metrics was identical, finding a ceiling at around 300 links, but the behavior of each line differed greatly. The unaltered algorithm gave us a behavior which asymptotically approaches the expected 300 links. The altered algorithm however gave us a linear growth which ceiling-ed at the expected 300 links maximum. This is caused by by a component of the new algorithm that when a node changed rings it had to destroy all of it's current links. this loss appears to cancel out the curve-behavior of the unaltered algorithm. The variance of the unaltered-algorithm was greater at every point in time then the altered algorithm in increased over time. This indicates the unaltered algorithm’s behavior was heavily influenced by the random starting conditions of the simulation where the constant variance of the altered algorithm indicated it is more stable in respect to starting topology. 